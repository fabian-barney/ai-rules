# CODE_REVIEW

Guidance for AI agents performing code and rules-document reviews.

## Scope
- Define review workflow, prioritization, and output expectations.
- Apply this file for PR/MR reviews and doc-rule audits.

## Semantic Dependencies
- Inherit review-layer boundary from `REVIEW/REVIEW.md`.
- Inherit dependency order and cross-cutting baseline from
  `CORE/RULE_DEPENDENCY_TREE.md`.
- Resolve applicable constraints through index docs:
  `LANGUAGE/LANGUAGE.md`, `DESIGN/DESIGN.md`,
  `ARCHITECTURE/ARCHITECTURE.md`, `FRAMEWORK/FRAMEWORK.md`,
  `LIBRARY/LIBRARY.md`, `BUILD_TOOLS/BUILD_TOOLS.md`,
  `INFRASTRUCTURE/INFRASTRUCTURE.md`, and `CI-CD/CI-CD.md`.
- Apply corresponding specialized leaf rules under `LANGUAGE/**`,
  `DESIGN/**`, `ARCHITECTURE/**`, `FRAMEWORK/**`, `LIBRARY/**`,
  `BUILD_TOOLS/**`, `INFRASTRUCTURE/**`, and `CI-CD/**`.
- Inherit security/testing/compliance constraints and priority direction from
  `SECURITY/SECURITY.md`, `TEST/TEST.md`, `COMPLIANCE/COMPLIANCE.md`.
- Severity levels are defined by this document's local severity model.

## Ruleset Read Gate (Mandatory)
- Start every review task by reading the complete ai-rules ruleset.
- "Complete ai-rules ruleset" means every Markdown file transitively reachable
  from the baseline entry point `AI.md`.
- In downstream-projects, also read every Markdown file transitively reachable
  from the downstream extension entry point described in
  `AI-RULES/DOWNSTREAM-PROJECT.md`.
- Do not skip reachable Markdown files and do not pick files ad-hoc.
- After the full read is complete, irrelevant rules may be removed from active
  context.

## Review Priority Order
1. Correctness and regression risk.
2. Security, privacy, and compliance.
3. Data integrity and error handling.
4. Architecture and boundary violations.
5. Performance and scalability risks.
6. Observability and operational readiness.
7. Maintainability/readability/test adequacy.

## Finding Severity Model
- `Critical`: exploitable security/data-loss/system outage risk.
- `High`: likely production failure or major correctness bug.
- `Medium`: maintainability/performance/reliability risk with non-trivial
  impact.
- `Low`: minor quality issue or consistency gap.

## Finding Format Requirements
Each finding should include:
- severity,
- impacted file/path reference,
- concrete issue description,
- why it matters (risk),
- actionable remediation guidance.

## Review Coverage Expectations
- Validate behavior changes against tests.
- Validate boundary adherence to semantic parent docs.
- Validate dependency additions for maturity/license/security fit.
- Validate error-handling and observability for failure paths.
- Validate migration/compatibility implications where relevant.

## Dependency Review Rules
- New dependencies require explicit justification.
- Reject niche/unmaintained dependencies unless strong rationale exists.
- Verify license compatibility with `COMPLIANCE/LICENSES.md`.
- Check overlap/redundancy with existing dependencies.

## Output Quality Rules
- Prioritize concrete findings over summaries.
- Avoid vague comments; anchor findings to code locations.
- Distinguish confirmed issues from assumptions/questions.
- State verification gaps explicitly (what was not tested/checked).

## High-Risk Pitfalls
1. Style-only feedback while missing correctness/security defects.
2. Vague findings without actionable remediation.
3. Missing boundary/dependency risk checks.
4. No distinction between critical risk and low-impact nits.
5. Ignoring test gaps for changed behavior.

## Do / Don't Examples
### 1. Finding Specificity
```text
Don't: "This looks wrong."
Do:    "High - `service/x.ts:87`: timeout is ignored; retries can hang request
        path under dependency outage. Add bounded timeout and failure mapping."
```

### 2. Priority Discipline
```text
Don't: spend review on naming nits while auth check is missing.
Do:    report missing auth as primary finding, then lower-priority nits.
```

### 3. Verification Transparency
```text
Don't: imply tests were run when they were not.
Do:    state which checks were run and what remains unverified.
```

## Review Checklist
- Are top-risk categories (correctness/security/compliance) covered first?
- Are findings severity-ranked and actionable?
- Are semantic parent-rule violations identified?
- Are dependency/tooling additions evaluated rigorously?
- Are test/verification gaps clearly documented?
- Is final output concise, concrete, and decision-useful?

## Override Notes
- Project-specific review policy may define extra gates, but severity-first,
  actionable, evidence-based review output here remains mandatory.
